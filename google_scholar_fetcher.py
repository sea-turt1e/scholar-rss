import hashlib
import json
import logging
import os
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Set

from serpapi import GoogleSearch

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)


class EfficientScholarFetcher:
    """APIä½¿ç”¨ã‚’æœ€å°é™ã«æŠ‘ãˆãŸåŠ¹ç‡çš„ãªè«–æ–‡å–å¾—ã‚¯ãƒ©ã‚¹ï¼ˆserpapiç‰ˆï¼‰"""

    def __init__(self, api_key: str, cache_dir: str = "./scholar_cache"):
        """
        Args:
            api_key: SerpApiã®APIã‚­ãƒ¼
            cache_dir: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¨å±¥æ­´ã‚’ä¿å­˜ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        self.api_key = api_key
        self.cache_dir = cache_dir
        self.history_file = os.path.join(cache_dir, "fetched_papers_history.json")
        self.api_usage_file = os.path.join(cache_dir, "api_usage.json")

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        os.makedirs(cache_dir, exist_ok=True)

        # å±¥æ­´ã¨APIä½¿ç”¨çŠ¶æ³ã‚’èª­ã¿è¾¼ã¿
        self.fetched_papers = self._load_history()
        self.api_usage = self._load_api_usage()

        # AIé–¢é€£ã®åŠ¹ç‡çš„ãªæ¤œç´¢ã‚¯ã‚¨ãƒªï¼ˆæ”¹å–„ç‰ˆï¼‰
        # æœ€æ–°è«–æ–‡é‡è¦–ã®å ´åˆï¼ˆåˆæœŸåŒ–æ™‚ã«å¹´ã‚’è¨­å®šï¼‰
        current_year = datetime.now().year
        self.search_queries_recent = [
            # arXivã®ã‚«ãƒ†ã‚´ãƒªãƒ™ãƒ¼ã‚¹ã®æ¤œç´¢ï¼ˆã‚ˆã‚ŠåŒ…æ‹¬çš„ï¼‰
            "site:arxiv.org (cat:cs.LG OR cat:cs.AI OR cat:cs.CL OR cat:cs.CV OR cat:cs.NE)",
            # # æ©Ÿæ¢°å­¦ç¿’é–¢é€£ã®å¹…åºƒã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            # 'site:arxiv.org ("deep learning" OR "neural network" OR "machine learning" OR "artificial intelligence")',
            # # ç‰¹å®šæ‰‹æ³•ãƒ»ãƒ¢ãƒ‡ãƒ«
            # 'site:arxiv.org ("transformer" OR "attention" OR "CNN" OR "RNN" OR "LSTM" OR "GAN")',
            # # è‡ªç„¶è¨€èªå‡¦ç†ãƒ»ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³
            # 'site:arxiv.org ("NLP" OR "computer vision" OR "image recognition" OR "text classification")',
            # # æœ€æ–°ã®AIæ‰‹æ³•
            # 'site:arxiv.org ("LLM" OR "large language model" OR "foundation model" OR "generative AI")',
            # # å¼·åŒ–å­¦ç¿’ãƒ»ãã®ä»–
            # 'site:arxiv.org ("reinforcement learning" OR "supervised learning" OR "unsupervised learning")',
        ]

        # é«˜å¼•ç”¨è«–æ–‡é‡è¦–ã®å ´åˆï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
        self.search_queries_quality = [
            # arXivã‚«ãƒ†ã‚´ãƒªãƒ™ãƒ¼ã‚¹ï¼ˆæœ€ã‚‚åŠ¹æœçš„ï¼‰
            "site:arxiv.org (cat:cs.LG OR cat:cs.AI OR cat:cs.CL OR cat:cs.CV)",
            # æ©Ÿæ¢°å­¦ç¿’ã®åŸºæœ¬æ¦‚å¿µ
            # 'site:arxiv.org ("machine learning" OR "deep learning" OR "neural network")',
            # # è‡ªç„¶è¨€èªå‡¦ç†
            # 'site:arxiv.org ("natural language processing" OR "NLP" OR "language model")',
            # # ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³
            # 'site:arxiv.org ("computer vision" OR "image processing" OR "object detection")',
            # # æœ€æ–°AIæŠ€è¡“
            # 'site:arxiv.org ("transformer" OR "attention mechanism" OR "generative AI")',
            # # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ãƒ»çµ±è¨ˆå­¦ç¿’
            # 'site:arxiv.org ("data mining" OR "statistical learning" OR "pattern recognition")',
        ]

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯é«˜å¼•ç”¨è«–æ–‡ã‚’å„ªå…ˆ
        self.search_queries = self.search_queries_quality

        # ç„¡æ–™è«–æ–‡ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ï¼ˆä¸»è¦ãªã‚‚ã®ã®ã¿ï¼‰
        self.free_domains = [
            "arxiv.org",
            # "openreview.net",
            # "aclanthology.org",
            # "proceedings.neurips.cc",
            # "proceedings.mlr.press",
            # "openaccess.thecvf.com",
            # "plos.org",
            # "ncbi.nlm.nih.gov/pmc",
            # "biorxiv.org",
            # "medrxiv.org",
        ]

    def fetch_papers_by_arxiv_categories(self, categories: List[str] = None, max_papers: int = 1) -> List[Dict]:
        """
        arXivã®ã‚«ãƒ†ã‚´ãƒªãƒ™ãƒ¼ã‚¹ã§è«–æ–‡ã‚’å–å¾—ï¼ˆã‚ˆã‚Šç¢ºå®Ÿã«MLé–¢é€£è«–æ–‡ã‚’å–å¾—ï¼‰

        Args:
            categories: å¯¾è±¡ã‚«ãƒ†ã‚´ãƒªã®ãƒªã‚¹ãƒˆ
            max_papers: å–å¾—ã™ã‚‹è«–æ–‡ã®æœ€å¤§æ•°

        Returns:
            å–å¾—ã—ãŸè«–æ–‡ã®ãƒªã‚¹ãƒˆ
        """
        if categories is None:
            categories = [
                "cs.LG",  # Machine Learning
                "cs.AI",  # Artificial Intelligence
                "cs.CL",  # Computation and Language
                "cs.CV",  # Computer Vision
                "cs.NE",  # Neural and Evolutionary Computing
                "stat.ML",  # Statistics - Machine Learning
            ]

        # ã‚«ãƒ†ã‚´ãƒªãƒ™ãƒ¼ã‚¹ã®ã‚¯ã‚¨ãƒªã‚’æ§‹ç¯‰
        category_query = " OR ".join([f"cat:{cat}" for cat in categories])
        query = f"site:arxiv.org ({category_query})"

        return self._search_with_query(query, max_papers)

    def _search_with_query(self, query: str, max_papers: int) -> List[Dict]:
        """
        æŒ‡å®šã•ã‚ŒãŸã‚¯ã‚¨ãƒªã§æ¤œç´¢ã‚’å®Ÿè¡Œ

        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            max_papers: å–å¾—ã™ã‚‹è«–æ–‡ã®æœ€å¤§æ•°

        Returns:
            å–å¾—ã—ãŸè«–æ–‡ã®ãƒªã‚¹ãƒˆ
        """
        today = datetime.now().strftime("%Y-%m-%d")

        # APIåˆ¶é™ãƒã‚§ãƒƒã‚¯
        can_use, remaining = self._check_api_limit()
        if not can_use:
            logger.error("Monthly API limit reached (100 calls)")
            return []

        current_year = datetime.now().year
        year_range = (current_year - 1, current_year)  # éå»1å¹´

        params = {
            "engine": "google_scholar",
            "q": query,
            "api_key": self.api_key,
            "num": 20,
            "as_ylo": year_range[0],
            "as_yhi": year_range[1],
            "hl": "en",
            "scisbd": 0,
        }

        logger.info(f"Searching with category-based query: {query}")

        try:
            search = GoogleSearch(params)
            results = search.get_dict()
            self._increment_api_usage()

            if "organic_results" not in results:
                logger.warning("No organic results found")
                return []

            # è«–æ–‡ã‚’å‡¦ç†
            papers = []
            for result in results["organic_results"]:
                if self._is_free_paper(result):
                    citations = self._extract_citations(result)
                    if citations >= 1:  # æœ€ä½1å¼•ç”¨
                        paper_info = {
                            "id": self._get_paper_id(result),
                            "title": result.get("title", ""),
                            "authors": self._extract_authors(result),
                            "year": self._extract_year(result),
                            "citations": citations,
                            "link": result.get("link", ""),
                            "pdf_link": self._extract_pdf_link(result),
                            "snippet": result.get("snippet", ""),
                            "fetched_date": today,
                        }
                        papers.append(paper_info)

            # å¼•ç”¨æ•°ã§ã‚½ãƒ¼ãƒˆ
            papers.sort(key=lambda x: x["citations"], reverse=True)

            # é‡è¤‡é™¤å»
            all_fetched_ids = set()
            for date_papers in self.fetched_papers.values():
                all_fetched_ids.update(date_papers)

            new_papers = []
            for paper in papers:
                if paper["id"] not in all_fetched_ids:
                    new_papers.append(paper)
                    if len(new_papers) >= max_papers:
                        break

            logger.info(f"Found {len(new_papers)} new ML papers")
            return new_papers

        except Exception as e:
            logger.error(f"Error in category-based search: {e}")
            return []

    def _load_history(self) -> Dict[str, Set[str]]:
        """å–å¾—æ¸ˆã¿è«–æ–‡ã®å±¥æ­´ã‚’èª­ã¿è¾¼ã¿"""
        if os.path.exists(self.history_file):
            with open(self.history_file, "r", encoding="utf-8") as f:
                data = json.load(f)
                return {date: set(papers) for date, papers in data.items()}
        return {}

    def _save_history(self):
        """å–å¾—æ¸ˆã¿è«–æ–‡ã®å±¥æ­´ã‚’ä¿å­˜"""
        data = {date: list(papers) for date, papers in self.fetched_papers.items()}
        with open(self.history_file, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def _load_api_usage(self) -> Dict[str, int]:
        """APIä½¿ç”¨çŠ¶æ³ã‚’èª­ã¿è¾¼ã¿"""
        if os.path.exists(self.api_usage_file):
            with open(self.api_usage_file, "r", encoding="utf-8") as f:
                return json.load(f)
        return {}

    def _save_api_usage(self):
        """APIä½¿ç”¨çŠ¶æ³ã‚’ä¿å­˜"""
        with open(self.api_usage_file, "w", encoding="utf-8") as f:
            json.dump(self.api_usage, f, indent=2)

    def _get_current_month_key(self) -> str:
        """ç¾åœ¨ã®æœˆã®ã‚­ãƒ¼ã‚’å–å¾—"""
        return datetime.now().strftime("%Y-%m")

    def _check_api_limit(self) -> tuple[bool, int]:
        """APIåˆ¶é™ã‚’ãƒã‚§ãƒƒã‚¯"""
        month_key = self._get_current_month_key()
        current_usage = self.api_usage.get(month_key, 0)
        remaining = 100 - current_usage
        return remaining > 0, remaining

    def _increment_api_usage(self):
        """APIä½¿ç”¨å›æ•°ã‚’å¢—ã‚„ã™"""
        month_key = self._get_current_month_key()
        self.api_usage[month_key] = self.api_usage.get(month_key, 0) + 1
        self._save_api_usage()

    def _get_paper_id(self, paper: Dict) -> str:
        """è«–æ–‡ã®ä¸€æ„ãªIDã‚’ç”Ÿæˆ"""
        content = f"{paper.get('title', '')}{paper.get('link', '')}"
        return hashlib.md5(content.encode()).hexdigest()

    def _is_free_paper(self, result: Dict) -> bool:
        """è«–æ–‡ãŒç„¡æ–™ã§ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        # PDFãƒªãƒ³ã‚¯ãŒã‚ã‚‹å ´åˆã‚’å„ªå…ˆãƒã‚§ãƒƒã‚¯
        if "resources" in result:
            for resource in result.get("resources", []):
                if "link" in resource:
                    link = resource["link"].lower()
                    # file_format = resource["file_format"].lower()
                    if (
                        link.endswith(".pdf")
                        or any(domain in link for domain in self.free_domains)
                        # or file_format == "pdf"
                    ):
                        return True

        # è«–æ–‡ãƒªãƒ³ã‚¯ã‚’ãƒã‚§ãƒƒã‚¯
        if "link" in result:
            link = result["link"].lower()
            if any(domain in link for domain in self.free_domains):
                return True

        # ã‚¿ã‚¤ãƒˆãƒ«ã«arXivè­˜åˆ¥å­ãŒã‚ã‚‹å ´åˆ
        if "title" in result:
            title_lower = result["title"].lower()
            if "arxiv:" in title_lower or "[arxiv]" in title_lower:
                return True

        return False

    def _extract_citations(self, result: Dict) -> int:
        """å¼•ç”¨æ•°ã‚’æŠ½å‡º"""
        if "inline_links" in result and "cited_by" in result.get("inline_links", {}):
            cited_by = result["inline_links"]["cited_by"]
            if "total" in cited_by:
                return cited_by["total"]
        return 0

    def _extract_year(self, result: Dict) -> Optional[int]:
        """è«–æ–‡ã®å¹´ã‚’æŠ½å‡º"""
        if "publication_info" in result and "summary" in result.get("publication_info", {}):
            summary = result["publication_info"]["summary"]
            import re

            year_match = re.search(r"\b(20[1-2]\d)\b", summary)
            if year_match:
                return int(year_match.group())
        return None

    def _extract_authors(self, result: Dict) -> List[str]:
        """è‘—è€…æƒ…å ±ã‚’æŠ½å‡º"""
        authors = []
        if "publication_info" in result and "authors" in result.get("publication_info", {}):
            for author in result["publication_info"]["authors"]:
                if "name" in author:
                    authors.append(author["name"])
        return authors

    def _extract_pdf_link(self, result: Dict) -> Optional[str]:
        """PDFãƒªãƒ³ã‚¯ã‚’æŠ½å‡ºï¼ˆå„ªå…ˆåº¦ä»˜ãï¼‰"""
        # PDFãƒªã‚½ãƒ¼ã‚¹ã‚’å„ªå…ˆ
        if "resources" in result:
            for resource in result["resources"]:
                if "link" in resource:
                    link = resource["link"]
                    if link.lower().endswith(".pdf"):
                        return link
                    # arXivãªã©ã®ç„¡æ–™ãƒªãƒã‚¸ãƒˆãƒª
                    if any(domain in link.lower() for domain in self.free_domains):
                        return link
        return None

    def _get_year_range_for_recent(self) -> tuple[int, int]:
        """æœ€æ–°è«–æ–‡æ¤œç´¢ç”¨ã®å¹´ç¯„å›²ã‚’å‹•çš„ã«è¨ˆç®—"""
        now = datetime.now()
        current_year = now.year
        current_month = now.month

        # å¹´ã®æœ€åˆã®3ãƒ¶æœˆé–“ã¯å‰å¹´ã‚‚å«ã‚ã‚‹
        if current_month <= 3:
            return (current_year - 1, current_year)
        else:
            return (current_year, current_year)

    def _update_search_queries_for_current_year(self):
        """ç¾åœ¨ã®å¹´ã«åˆã‚ã›ã¦æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’æ›´æ–°"""
        current_year = datetime.now().year

        # æœ€æ–°è«–æ–‡é‡è¦–ã®ã‚¯ã‚¨ãƒªã‚’ç¾åœ¨ã®å¹´ã§æ›´æ–°ï¼ˆã‚ˆã‚ŠåŒ…æ‹¬çš„ï¼‰
        self.search_queries_recent = [
            f"site:arxiv.org (cat:cs.LG OR cat:cs.AI OR cat:cs.CL OR cat:cs.CV) year:{current_year}",
            f'site:arxiv.org ("machine learning" OR "deep learning") year:{current_year}',
            f'site:arxiv.org ("transformer" OR "attention") year:{current_year}',
            f'site:arxiv.org ("LLM" OR "large language model") year:{current_year}',
            f'site:arxiv.org "generative AI" year:{current_year}',
        ]

    def fetch_daily_papers(self, force: bool = False, prefer_recent: bool = False, max_papers: int = 1) -> List[Dict]:
        """
        1æ—¥ã®è«–æ–‡ã‚’å–å¾—ï¼ˆMachine Learningé–¢é€£ã‚’åŒ…æ‹¬çš„ã«ï¼‰

        Args:
            force: ä»Šæ—¥æ—¢ã«å–å¾—æ¸ˆã¿ã§ã‚‚å¼·åˆ¶çš„ã«å–å¾—ã™ã‚‹ã‹
            prefer_recent: æœ€æ–°è«–æ–‡ã‚’å„ªå…ˆã™ã‚‹ã‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯é«˜å¼•ç”¨è«–æ–‡å„ªå…ˆï¼‰
            max_papers: å–å¾—ã™ã‚‹è«–æ–‡ã®æœ€å¤§æ•°

        Returns:
            å–å¾—ã—ãŸè«–æ–‡ã®ãƒªã‚¹ãƒˆ
        """
        today = datetime.now().strftime("%Y-%m-%d")

        # æ—¢ã«ä»Šæ—¥ã®åˆ†ã‚’å–å¾—æ¸ˆã¿ã‹ãƒã‚§ãƒƒã‚¯
        if today in self.fetched_papers and not force:
            logger.info(f"Already fetched papers for {today}")
            return []

        # APIåˆ¶é™ãƒã‚§ãƒƒã‚¯
        can_use, remaining = self._check_api_limit()
        if not can_use:
            logger.error("Monthly API limit reached (100 calls)")
            return []

        logger.info(f"API calls remaining this month: {remaining}")

        # ã¾ãšã‚«ãƒ†ã‚´ãƒªãƒ™ãƒ¼ã‚¹ã§ç¢ºå®Ÿã«MLè«–æ–‡ã‚’å–å¾—
        category_papers = self.fetch_papers_by_arxiv_categories(max_papers=max_papers)

        if category_papers:
            # å±¥æ­´ã«è¿½åŠ 
            self.fetched_papers[today] = {p["id"] for p in category_papers}
            self._save_history()
            self._save_daily_papers(today, category_papers)
            logger.info(f"Successfully fetched {len(category_papers)} papers via category search")
            return category_papers

        # ã‚«ãƒ†ã‚´ãƒªãƒ™ãƒ¼ã‚¹ã§å–å¾—ã§ããªã‹ã£ãŸå ´åˆã¯å…ƒã®ãƒ­ã‚¸ãƒƒã‚¯ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        logger.info("Falling back to keyword-based search")
        keyword_papers = self._fetch_papers_keyword_based(force, prefer_recent, max_papers)

        # å±¥æ­´ã«è¿½åŠ 
        if keyword_papers:
            self.fetched_papers[today] = {p["id"] for p in keyword_papers}
            self._save_history()
            self._save_daily_papers(today, keyword_papers)

        return keyword_papers

    def _fetch_papers_keyword_based(self, force: bool, prefer_recent: bool, max_papers: int) -> List[Dict]:
        """
        å…ƒã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®æ¤œç´¢ãƒ­ã‚¸ãƒƒã‚¯
        """
        today = datetime.now().strftime("%Y-%m-%d")

        # æ¤œç´¢æˆ¦ç•¥ã‚’é¸æŠ
        if prefer_recent:
            # ç¾åœ¨ã®å¹´ã«åˆã‚ã›ã¦ã‚¯ã‚¨ãƒªã‚’æ›´æ–°
            self._update_search_queries_for_current_year()
            queries = self.search_queries_recent
            year_range = self._get_year_range_for_recent()  # å‹•çš„ã«è¨ˆç®—
            min_citations = 1  # æœ€æ–°è«–æ–‡ã¯å¼•ç”¨æ•°ãŒå°‘ãªã„ãŸã‚
            logger.info(f"Using recent papers strategy: {year_range[0]}-{year_range[1]}")
        else:
            queries = self.search_queries_quality
            current_year = datetime.now().year
            year_range = (current_year - 2, current_year)  # éå»2å¹´
            min_citations = 10  # è³ªã‚’ä¿è¨¼

        # æœ€é©ãªã‚¯ã‚¨ãƒªã‚’é¸æŠï¼ˆæ¯æ—¥ç•°ãªã‚‹ã‚¯ã‚¨ãƒªã‚’ä½¿ã†ï¼‰
        day_of_month = datetime.now().day
        query_index = (day_of_month - 1) % len(queries)
        query = queries[query_index]

        params = {
            "engine": "google_scholar",
            "q": query,
            "api_key": self.api_key,
            "num": 20,  # 1å›ã§20ä»¶å–å¾—
            "as_ylo": year_range[0],
            "as_yhi": year_range[1],
            "hl": "en",
            "scisbd": 0,
        }

        logger.info(f"Searching with query: {query} (Strategy: {'recent' if prefer_recent else 'quality'})")
        try:
            # serpapi ã‚’ä½¿ã£ã¦æ¤œç´¢
            search = GoogleSearch(params)
            results = search.get_dict()

            # APIä½¿ç”¨å›æ•°ã‚’è¨˜éŒ²
            self._increment_api_usage()

            if "organic_results" not in results:
                logger.warning("No organic results found")
                return []

            # ç„¡æ–™è«–æ–‡ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            free_papers = []

            for result in results["organic_results"]:
                if self._is_free_paper(result):
                    citations = self._extract_citations(result)

                    # è¢«å¼•ç”¨æ•°ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                    if citations >= min_citations:
                        paper_info = {
                            "id": self._get_paper_id(result),
                            "title": result.get("title", ""),
                            "authors": self._extract_authors(result),
                            "year": self._extract_year(result),
                            "citations": citations,
                            "link": result.get("link", ""),
                            "pdf_link": self._extract_pdf_link(result),
                            "snippet": result.get("snippet", ""),
                            "fetched_date": today,
                        }
                        free_papers.append(paper_info)

            # è¢«å¼•ç”¨æ•°ã§é™é †ã‚½ãƒ¼ãƒˆ
            free_papers.sort(key=lambda x: x["citations"], reverse=True)
            logger.info(f"Found {len(free_papers)} free papers with {min_citations}+ citations")

            # éå»ã«å–å¾—ã—ã¦ã„ãªã„è«–æ–‡ã®ã¿ã‚’é¸æŠ
            all_fetched_ids = set()
            for date_papers in self.fetched_papers.values():
                all_fetched_ids.update(date_papers)

            new_papers = []
            for paper in free_papers:
                if paper["id"] not in all_fetched_ids:
                    new_papers.append(paper)
                    if len(new_papers) >= max_papers:
                        break

            logger.info(f"Selected {len(new_papers)} new papers for today")

            return new_papers

        except Exception as e:
            logger.error(f"Error fetching papers: {e}")
            return []

    def _save_daily_papers(self, date: str, papers: List[Dict]):
        """ãã®æ—¥ã®è«–æ–‡ã‚’ä¿å­˜"""
        filename = os.path.join(self.cache_dir, f"papers_{date}.json")
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(papers, f, ensure_ascii=False, indent=2)
        logger.info(f"Saved {len(papers)} papers to {filename}")

    def get_monthly_summary(self) -> Dict:
        """ä»Šæœˆã®å–å¾—çŠ¶æ³ã‚µãƒãƒªãƒ¼"""
        month_key = self._get_current_month_key()
        api_usage = self.api_usage.get(month_key, 0)

        # ä»Šæœˆå–å¾—ã—ãŸè«–æ–‡æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        papers_count = 0
        for date, papers in self.fetched_papers.items():
            if date.startswith(month_key):
                papers_count += len(papers)

        # æ®‹ã‚Šæ—¥æ•°ã§å–å¾—å¯èƒ½ãªè«–æ–‡æ•°ã‚’è¨ˆç®—
        days_remaining = (datetime.now().replace(day=1) + timedelta(days=32)).replace(day=1) - datetime.now()
        potential_papers = min(days_remaining.days * 3, (100 - api_usage) * 3)

        return {
            "month": month_key,
            "api_calls_used": api_usage,
            "api_calls_remaining": 100 - api_usage,
            "papers_fetched": papers_count,
            "efficiency": f"{papers_count / max(api_usage, 1):.2f} papers/API call",
            "potential_papers_this_month": papers_count + potential_papers,
        }

    def print_papers(self, papers: List[Dict]):
        """è«–æ–‡æƒ…å ±ã‚’è¦‹ã‚„ã™ãè¡¨ç¤º"""
        if not papers:
            print("No new papers found today.")
            return

        print(f"\n=== Today's Top {len(papers)} AI Papers ===\n")

        for i, paper in enumerate(papers, 1):
            print(f"{i}. {paper['title']}")
            if paper["authors"]:
                authors_str = ", ".join(paper["authors"][:3])
                if len(paper["authors"]) > 3:
                    authors_str += f" ... +{len(paper['authors']) - 3} authors"
                print(f"   Authors: {authors_str}")
            print(f"   Year: {paper['year'] or 'Unknown'}")
            print(f"   Citations: {paper['citations']} ğŸ”¥")
            print(f"   Link: {paper['link']}")
            if paper["pdf_link"]:
                print(f"   PDF: {paper['pdf_link']} ğŸ“„")
            print(f"   Summary: {paper['snippet'][:150]}...")
            print()

    def get_all_fetched_papers(self, days: int = 30) -> List[Dict]:
        """éå»næ—¥é–“ã®å–å¾—æ¸ˆã¿è«–æ–‡ã‚’å–å¾—"""
        papers = []
        start_date = datetime.now() - timedelta(days=days)

        for date_str in sorted(self.fetched_papers.keys(), reverse=True):
            date = datetime.strptime(date_str, "%Y-%m-%d")
            if date >= start_date:
                filename = os.path.join(self.cache_dir, f"papers_{date_str}.json")
                if os.path.exists(filename):
                    with open(filename, "r", encoding="utf-8") as f:
                        daily_papers = json.load(f)
                        papers.extend(daily_papers)

        return papers


# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    from dotenv import load_dotenv

    load_dotenv()
    # APIã‚­ãƒ¼ã‚’è¨­å®š
    API_KEY = os.getenv("SERP_API_KEY")

    # ãƒ•ã‚§ãƒƒãƒãƒ£ãƒ¼ã‚’åˆæœŸåŒ–
    fetcher = EfficientScholarFetcher(API_KEY)

    # ä»Šæ—¥ã®è«–æ–‡ã‚’å–å¾—
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼šé«˜å¼•ç”¨è«–æ–‡å„ªå…ˆï¼ˆè³ªé‡è¦–ï¼‰
    # papers = fetcher.fetch_daily_papers()

    # æœ€æ–°è«–æ–‡ã‚’å–å¾—ã—ãŸã„å ´åˆ
    papers = fetcher.fetch_daily_papers(prefer_recent=True)

    # çµæœã‚’è¡¨ç¤º
    fetcher.print_papers(papers)

    # æœˆæ¬¡ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
    summary = fetcher.get_monthly_summary()
    print("\n=== Monthly Summary ===")
    print(f"Month: {summary['month']}")
    print(f"API calls used: {summary['api_calls_used']}/100")
    print(f"Papers fetched: {summary['papers_fetched']}")
    print(f"Efficiency: {summary['efficiency']}")
    print(f"Potential papers this month: {summary['potential_papers_this_month']}")

    # éå»30æ—¥é–“ã®è«–æ–‡ä¸€è¦§ã‚’å–å¾—ã™ã‚‹ä¾‹
    print("\n=== Recent Papers (Last 30 days) ===")
    recent_papers = fetcher.get_all_fetched_papers(days=30)
    print(f"Total papers collected: {len(recent_papers)}")

    # cronã‚„ã‚¿ã‚¹ã‚¯ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã§æ¯æ—¥å®Ÿè¡Œã™ã‚‹å ´åˆã®ä¾‹
    # 0 9 * * * /usr/bin/python3 /path/to/your_script.py >> /path/to/logs/scholar.log 2>&1

---
title: "【論文要約】 HiPhO: How Far Are (M) LLMs from Humans in the Latest High School Physics Olympiad Benchmark?"
tags:
  - "機械学習"
  - "AI"
  - "論文"
  - "arXiv"
private: false
updated_at: ""
id: null
organization_url_name: null
slide: false
ignorePublish: false
---

## 論文情報

- **著者**: F Yu, H Wan, Q Cheng, Y Zhang, J Chen, F Han
- **論文概要リンク**: https://arxiv.org/abs/2509.07894
- **論文PDFリンク**: https://arxiv.org/pdf/2509.07894?

## 要約

本論文は、高校物理オリンピック（Physics Olympiad）を対象に最新の物理問題に対する（多モーダル）大規模言語モデル（(M)LLMs）の能力を評価するための初のベンチマーク「HIPHO」を提案する。HIPHOは2024-2025年の13回のオリンピック試験を網羅し、テキストのみから図表を含む出題まで多様なモーダリティを扱い、公式の採点方式に基づく細かなステップレベル評価を行う。30種類の最先端モデルを評価した結果、クローズドソースの推論型MLLMは6～12個のゴールドメダルを獲得しているが、トップの人間にはまだ及ばず、オープンソースのモデルには大きな差があることを示した。

## 主要なポイント

1. 物理オリンピック問題を対象にした最初の包括的かつ最新の（2024-2025年）多モーダルベンチマーク「HIPHO」を構築。
2. 公式採点基準に基づく詳細な答えとステップレベルの評価を行い、人間の試験採点に極めて近い評価体制を実現。
3. モデルの得点をリアルな人間のメダル獲得基準と比較し、人間とモデルの実力差を定量化。
4. クローズドソース推論型MLLMは多くの試験でゴールドメダルに相当する成績を収めるが、トップ人間には依然到達していない。
5. オープンソースのチャット型MLLMはほとんどがブロンズ以下、推論型は一部でゴールドを取得し始めているが、完全な追従には程遠い。


## メソッド

- **データ収集**
2024-2025年に実施された13の高校物理オリンピック（国際・地域）の公式試験問題を収集。問題はテキスト、図解付き、変数付き図、データ図の4種モーダリティに分類され、高度な物理分野（力学、熱力学、現代物理、電磁気、光学）にもラベル付け。
- **データ処理**
PDFからMarkdownへ変換、OCR誤り修正、人手によるQA対応付けと検証、問題および解答への単位指定、補完・構造化（文脈統合・小問分離）を実施。
- **評価方法**
- 公式の採点基準に基づくステップレベルの部分点付与を導入。
- 数式や解答の形式の違いを越えて正誤を判定するため、強力な判定モデル (Gemini-2.5-Flash) による自動採点を採用。
- 各問題の得点を合算し、合計得点を人間参加者のメダル基準と比較。
- **モデル評価**
- 30の最先端（M）LLMsを評価。クローズドソース・オープンソース、推論型・チャット型問わず幅広く評価し、複数回推論を行い平均得点算出。

## 意義・影響

- 物理オリンピックの厳密な試験問題を用い、最新かつ多様なモーダリティを含んだ物理問題に対する（M）LLMsの推論性能を体系的に評価した世界初のベンチマークを構築。
- 公式採点ルールに即したステップレベル評価により、人間の採点とほぼ同等の品質の評価基盤を提供。これによりAIモデルと人間間の公平かつ正確な比較が可能になった。
- 結果として、最先端クローズドソースMLLMの強力な物理推論能力と、依然存在する人間トップとのギャップを明確に示し、物理理解・推論AIの今後の技術進展方向を示唆。
- オープンソースコミュニティの進展も注目され、そのポテンシャルを示すと同時に、さらなる改善の必要性も浮き彫りに。
- 継続的に新たな問題や採点データを追加可能な拡張性を持ち、将来的に中国や米国など他の主要物理オリンピックも統合し、標準的かつ信頼性の高い物理推論ベンチマークとしての活用が期待される。

---

以上が本論文「HiPhO: How Far Are (M) LLMs from Humans in the Latest High School Physics Olympiad Benchmark?」の詳細な日本語要約です。


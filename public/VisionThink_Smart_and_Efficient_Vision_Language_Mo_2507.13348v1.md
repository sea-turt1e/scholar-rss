---
title: "[arXiv] VisionThink: Smart and Efficient Vision Language Model via Reinforcement
  Learning"
tags:
  - "機械学習"
  - "AI"
  - "論文"
  - "arXiv"
  - "Python"
private: false
updated_at: ""
id: null
organization_url_name: null
slide: false
ignorePublish: false
---

## 論文情報

- **著者**: Senqiao Yang, Junyi Li, Xin Lai, Bei Yu, Hengshuang Zhao, Jiaya Jia
- **arXiv ID**: [2507.13348v1](http://arxiv.org/abs/2507.13348v1)
- **PDF**: [Link](https://arxiv.org/pdf/2507.13348v1.pdf)

## 要約

VisionThinkは、視覚言語モデル（VLM）における視覚トークンの効率的な処理を実現する新しい手法です。従来のVLMは全ての画像に対して大量の視覚トークンを使用していましたが、多くの実世界タスクではそこまで多くのトークンは不要であることに着目しました。本手法は最初に低解像度画像から処理を開始し、必要に応じて高解像度画像を動的に要求することで、タスクに応じた適応的な視覚トークン数の調整を実現しています。強化学習とLLM-as-Judge戦略を用いてモデルを訓練し、OCR関連タスクでは高精度を維持しながら、簡単なタスクでは大幅な計算量削減を達成しました。

## 主要なポイント

1. **動的解像度処理**: 低解像度画像から開始し、必要に応じて特別なトークンを出力して高解像度画像を要求する適応的なアプローチを提案
2. **強化学習による最適化**: LLM-as-Judge戦略を用いた強化学習により、固定的な閾値ではなくケースバイケースでトークン圧縮を判断するモデルを構築
3. **効率性と精度の両立**: OCR関連タスクでは高精度を維持しつつ、一般的なVQAタスクでは1/4解像度でも十分な性能を示し、大幅な計算量削減を実現

## 意義・影響

この研究は視覚言語モデルの実用化において重要な進歩を示しており、計算リソースを効率的に活用しながら高精度を維持する新たなパラダイムを提案しています。特に、タスクの複雑さに応じて動的にリソース配分を調整する考え方は、今後のマルチモーダルAIシステムの設計指針として広く応用される可能性があり、モバイルデバイスやエッジコンピューティング環境での実装においても大きな価値を持つと考えられます。

